# Ceph 配置概述

Ceph 配置的官方文档：https://ceph.readthedocs.io/en/latest/rados/configuration/

共分两部分，一部分是对象存储配置，一部分是优化集群性能的配置。



## 储存相关的组件

有两个守护进程用户用于将数据储存进磁盘：

**Ceph OSDs** ：一个 OSD 对应一块磁盘，OSD也可以由多种设备组合来支持，例如用于大多数数据的HDD和用于某些元数据的SSD（或SSD的分区），OSD 的数量决定了储存的数据量，OSD 储存了原始数据及冗余（复制或纠删码）。

**Ceph Monitor**：用于管理集群状态，例如群集成员身份和身份验证信息，对于较小的群集，只需要几GB的容量，但是对于较大的群集，监控器数据库可以达到数十或可能数百GB的容量。

OSD 有两种方式管理数据，一种是 BlueStore，另外一种是 FileStore，从 Luminous 12.2.z 版本之后，默认就是 BlueStore 了，BuleStore 接管裸设备，所以性能更好一些。



## BlueStore

BlueStore的主要功能包括：

- 直接管理存储设备。 BlueStore使用原始块设备或分区。这样可以避免任何可能影响性能或增加复杂性的中间抽象层（例如XFS等本地文件系统）。
- 使用 RocksDB 作为元数据管理。嵌入 RocksDB 的键/值数据库是为了管理内部元数据，例如从对象名称到磁盘上块位置的映射。（TiKV 也在用 RocksDB）
- 完整的数据和元数据校验和。默认情况下，所有写入BlueStore的数据和元数据都受到一个或多个校验和的保护。未经验证，不会从磁盘读取任何数据返回给用户。
- 内部压缩。写入的数据在写入磁盘之前可以选择压缩。
- 多设备元数据分层。 BlueStore允许将其内部日志（预写日志）写入单独的高速设备（例如SSD，NVMe或NVDIMM），以提高性能。如果有大量的快速存储可用，内部元数据也可以存储在更快的设备上。
- 高效的写时复制。 RBD 和 CephFS 快照依赖于在 BlueStore 中有效实现的写时复制克隆机制。这将为常规快照和擦除代码池（依赖克隆实现高效的两阶段提交）提供高效的IO。





























